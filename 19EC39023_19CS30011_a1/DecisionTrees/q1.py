# -*- coding: utf-8 -*-
"""Assignment1_ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14-rU_G9zGXkKUGaTLUq3brX3CdDFkdJm
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sys
"""#Assignment 1 Begins

"""

# Setting the ouput in a text file
fptr=open('outputFile.txt','w')

sys.stdout=fptr

# Readind the Data as Input 
data=pd.read_csv('Train_D_Tree.csv')
print("\n\n---- Original Data ----\n")
print(data)

# Renaming the Columns for Ease of access
data.columns=['restaurant','cheese','mushroom','inch','spicy','price']
print("\n\n---- Data After Changing Columns ----\n")
print(data)

# Assigning yes -> 1 and no -> 0

data['cheese']=pd.Series(np.where(data.cheese.values=='yes',1,0),data.index)
data['mushroom']=pd.Series(np.where(data.mushroom.values=='yes',1,0),data.index)
data['spicy']=pd.Series(np.where(data.spicy.values=='yes',1,0),data.index)

print("\n\n---- Making the data in the required format ----\n")
print(data)

# Checking the Data dtypes of the data
print("\n\n---- Checking the Data Types of the Columns -----\n")
print(data.dtypes)

"""Calculating the Best Split Function"""

# Checking the Best split criteria
# based on the reduction in mean squared error

def calculateSplit(currData):
  maxChange=-1e18
  index=-1
  cond=1e18
  leaf=True
  for i in range(len(currData.columns)-1):

    # Taking Unique Values to know where to split
    val=currData[currData.columns[i]].sort_values().unique()[1:]
    for j in range(len(val)):
      t1=currData[currData[currData.columns[i]]<val[j]]
      t2=currData[currData[currData.columns[i]]>=val[j]]
      
      # If the partition is a leaf node 
      if t1.shape[0]==0 or t2.shape[0]==0:
        maxChange=currData[currData.columns[-1]].var()*currData.shape[0]
        index=i
        cond=val[j]
        leaf=True
        continue
      
      # Checking the change in sum of square error
      sum_error=currData[currData.columns[-1]].var()*currData.shape[0]
      sum_error_child1=t1[t1.columns[-1]].var()*t1.shape[0]
      sum_error_child2=t2[t2.columns[-1]].var()*t2.shape[0]
     
      entropyError=sum_error-(sum_error_child1+sum_error_child2)

      #Updating with the maximum change in error
      if entropyError>maxChange :
        maxChange=entropyError
        index=i
        cond=val[j]
        leaf=False

  
  # Returning the column index, condition for splitting , the maximum change, whether 
  # the current node is a leaf or not
  return index,cond,maxChange,leaf

"""Training the Decision Tree Function"""

# Function to Train the tree

def trainTree(currData,depth):
  # index of column, condition of split, curr square error, left child, right child
  subTree=dict() 

  # Getting the best Split
  col_index,cond,square_error,leaf=calculateSplit(currData)

  # Checking is current node is the leaf
  if leaf:
    subTree['col_index']=col_index
    subTree['condition']=cond
    subTree['square_error']=square_error
    subTree['leaf']=leaf
    subTree['left_child']=-1
    subTree['right_child']=-1
    subTree['depth']=depth
    return subTree,depth
  
  # Storing the required data
  # Recursively splitting the nodes further

  subTree['col_index']=col_index
  subTree['condition']=cond
  subTree['square_error']=square_error
  subTree['leaf']=leaf
  subTree['depth']=depth
  term1,dep1=trainTree(currData[currData[currData.columns[col_index]]<cond],depth+1)
  term2,dep2=trainTree(currData[currData[currData.columns[col_index]]>=cond],depth+1)

  subTree['left_child']=term1
  subTree['right_child']=term2
  
  # Returning the stored node split and the depth
  return subTree,max(dep1,dep2,depth)

# Predicts the answer using the trained decision Tree
# On the provided data
# Tree pruning is performed bt specifying the Depth

def DecisionTree_Predictor(subTree,currData,predictData,max_depth):
  # Returns the mean if Tree maxdepth is reached
  if(subTree['depth']==max_depth):
    return currData[currData.columns[-1]].mean()
  
  # Returns the mean if the leaf is reached
  if(subTree['leaf']):
    return currData[currData.columns[-1]].mean()
  
  # Returns the answer based on the tree splitting criteria
  if(predictData[subTree['col_index']]<subTree['condition']):
      childData=currData[currData[currData.columns[subTree['col_index']]]<subTree['condition']]
      return DecisionTree_Predictor(subTree['left_child'],childData,predictData,max_depth)
  else:
      childData=currData[currData[currData.columns[subTree['col_index']]]>=subTree['condition']]
      return DecisionTree_Predictor(subTree['right_child'],childData,predictData,max_depth)

# Function to Print the Tree

def printTree(subTree,maxDepth):
  spaceCnt=subTree['depth']
  prefixString="|"
  while spaceCnt>0:
    prefixString+="\t|"
    spaceCnt=spaceCnt-1
  prefixString+="---"
  if subTree['leaf'] or maxDepth==subTree['depth']:
    print(prefixString+" Mean Value")
    return
  
  print(prefixString+f"feature_{subTree['col_index']} < {subTree['condition']}")

  printTree(subTree['left_child'],maxDepth)
  print(prefixString+f"feature_{subTree['col_index']} >= {subTree['condition']}")

  printTree(subTree['right_child'],maxDepth)

"""Testing Tree Performance"""

# Running the code for 10 iterations on a random subset of data
# Taking the bes regression Tree obtained

varyingError=[]
bestTree=None
minError=1e18
depthTree=-1

for iter in range(10):
  rng = np.random.default_rng()
  rVal=rng.integers(low=1,high=10,size=data.shape[0])
  msk=rVal<=7
  train_Data=data[msk]
  msk=~msk
  test_Data=data[msk]
  decisionTree,mxDepth=trainTree(train_Data,0)
  y=[]

  for i in range(test_Data.shape[0]):
    currData=test_Data.iloc[i,:]
    y.append(DecisionTree_Predictor(decisionTree,train_Data,currData,100))
  
  y=np.array(y)
  y=y-test_Data['price']
  y=np.abs(y)
  y=y*y
  val=np.sqrt(y.mean())
  if(val<minError):
    bestTree=decisionTree
    minError=val
    depthTree=mxDepth
  varyingError.append(val)

plt.figure(1)
plt.title("Root Mean Squared Error Vs Random Data Split")
xAxis=np.array(range(1,11))
plt.plot(xAxis,varyingError)
plt.xlabel("Iteration Number")
plt.ylabel("Root Mean Squared Error")


print("\n\n---- Best Obtained Tree ----\n")
print("Min Error Obtained in the Best Tree : ",minError)
print("Depth of the Best Tree Obtained: ",depthTree)

print("\n\n---- Pruning the Best Tree Depth ----\n")
varyingError=[]
for allowedDepth in range(depthTree+1):
  y=[]
  print(f"\n\nDepth of Tree {allowedDepth}\n")
  for i in range(data.shape[0]):
    currData=data.iloc[i,:]
    y.append(DecisionTree_Predictor(bestTree,data,currData,allowedDepth))
  
  y=np.array(y)
  y=y-data['price']
  y=np.abs(y)
  y=y*y
  val=np.sqrt(y.mean())
  varyingError.append(val)

  print(f"\n-- Tree Depth {allowedDepth}--\n")
  printTree(bestTree,allowedDepth)


plt.figure(2)
plt.title("Mean Squared Error Vs Depth of Tree")
xAxis=np.array(range(0,depthTree+1))
plt.plot(xAxis,varyingError)
plt.xlabel("Depth of Tree")
plt.ylabel("Mean Squared Error")
plt.show()

fptr.close()